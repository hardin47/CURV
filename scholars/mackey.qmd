---
title: "Lester Mackey"
layout: default
output:
  html_document:
    keep_md: yes
---

::: {.grid}

::: {.g-col-4}
```{r fig.cap = "Lester Mackey", fig.alt = "Image credit: Dana J Quigley", preview = TRUE, echo = FALSE}
knitr::include_graphics("../images/mackey.jpeg")
```
:::

:::{.g-col-8}
# Lester Mackey

Dr. Mackey is a machine learning researcher at Microsoft Research New England and an adjunct professor at Stanford University.  His PhD (Computer Science 2012) and MA (Statistics 2011) are both from University of California, Berkeley, while his undergraduate degree (Computer Science 2007) is from Princeton University. 

He is involved in Stanford's initiative of <a href = "https://stats-for-good.stanford.edu/" target = "_blank">Statistics for Social Good</a> and has the following quote on his website:

>Quixotic though it may sound, I hope to use computer science and statistics to change the world for the better.


In 2023, Dr. Mackey was awarded a <a href = "https://www.macfound.org/fellows/class-of-2023/lester-mackey" target = "_blank">MacArthur Genius Grant</a>.
:::

:::


#### Topics covered

From Dr. Mackey's <a href = "http://stanford.edu/~lmackey/" target = "_blank">personal website</a> his areas of research are:

* statistical machine learning
* scalable algorithms
* high-dimensional statistics
* approximate inference
* probability

#### Relevant work

* Koulik Khamaru, Yash Deshpande, Lester Mackey, and Martin J. Wainwright, <a href = "https://arxiv.org/pdf/2107.02266.pdf" target = "_blank">Near-optimal inference in adaptive linear regression</a>

> When data is collected in an adaptive manner, even simple methods like ordinary least squares can exhibit non-normal asymptotic behavior. As an undesirable consequence, hypothesis tests and confidence intervals based on asymptotic normality can lead to erroneous results. We propose a family of online debiasing estimators to correct these distributional anomalies in least squares estimation. Our proposed methods take advantage of the covariance structure present in the dataset and provide sharper estimates in directions for which more information has accrued. We establish an asymptotic normality property for our proposed online debiasing estimators under mild conditions on the data collection process and provide asymptotically exact confidence intervals...

* Pierre Bayle, Alexandre Bayle, Lucas Janson, and Lester Mackey, <a href = "https://proceedings.neurips.cc/paper/2020/hash/bce9abf229ffd7e570818476ee5d7dde-Abstract.html" target = "_blank">Cross-validation Confidence Intervals for Test Error</a> **Advances in Neural Information Processing Systems (NeurIPS)**, December 2020.

> This work develops central limit theorems for cross-validation and consistent estimators of its asymptotic variance under weak stability conditions on the learning algorithm. Together, these results provide practical, asymptotically-exact confidence intervals for k-fold test error and valid, powerful hypothesis tests of whether one learning algorithm has smaller k-fold test error than another. These results are also the first of their kind for the popular choice of leave-one-out cross-validation. In our real-data experiments with diverse learning algorithms, the resulting intervals and tests outperform the most popular alternative methods from the literature...


#### Outside links

* [Mathematically Gifted & Black](https://mathematicallygiftedandblack.com/honorees/lester-mackey/)
* [Linkedin](https://www.linkedin.com/in/lester-mackey-5902909/)
* [personal](http://stanford.edu/~lmackey/)


####  Other

The precursor to <a href = "https://www.kaggle.com/" target = "_blank">kaggle</a> was a <a href = "https://en.wikipedia.org/wiki/Netflix_Prize" target = "_blank">$1 million prize given by Netflix</a> to the most accurate prediction of ratings that people give to the movies they watch.  As undergraduates, Dr. Mackey and two friends led the competition for a few hours in its first year.  Later, groups merged and Dr. Mackey's group merged with a few others, forming The Ensemble.  Their final analysis came in second with the **exact same** error rates as the winning entry.  The winning entry, however, had been submitted 20 minutes prior.  Sigh.

----------------------

[Back to the full database](https://hardin47.github.io/CURV/)

[GitHub repository](https://github.com/hardin47/CURV/)

